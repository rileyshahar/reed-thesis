[TODO: An introduction to the chapter;
cite~\cite{katz-lindell-2014, pass-shelat-2020, rosulek-2021}.]

\section{Foundations}

\subsection{One-Way Functions}

Many cryptographic protocols rely on \emph{one-way functions}, which are
informally functions that are easy to compute, but hard to invert. The former
notion is easy to formalize in terms of time complexity, but the latter is more
difficult. We typically ask that any ``reasonably efficient'' algorithm---called
the \emph{adversary}---attempting to invert the function has a negligible chance
of success.

In computer science, we generally assume that algorithms are efficient if and
only if they are polynomial-time; this assumption has been borne out by decades
of practice. This motivates our definition of a ``negligible'' chance: we say
that a function $f$ is \emph{negligible} if $f(n) = o(n^{-k})$
for every $k$; in other words, if it is asymptotically smaller than any rational
function. In this case, we write $f = \negl$ or just $f = \negl[]$. The set
of negligible functions has all the nice closure properties we expect; in
particular, the sum of negligible functions is negligible.

% The difficulty is in determining which adversaries are ``reasonable''. We
% generally ask that adversaries are non-uniform probabilistic polynomial-time
% algorithms. The non-uniformity primarily serves to simplify proofs, by allowing
% us to not worry about the size of the adversary.

\begin{ntn}
  We will use PPT as shorthand for probabilistic polynomial-time. When we refer
  to an \emph{adversary}, \emph{distinguisher}, or \emph{simulator}, we always
  mean a non-uniform PPT algorithm\footnote{Both PPT and non-uniform PPT are defined
  in~\Cref{chap:cs-theory}.}.
\end{ntn}

\begin{dfn}[one-way function]\label{def:one-way function}
  A function $f$ is \emph{one-way} if:
  \begin{itemize}
    \item (easy to compute) $f$ is PPT-computable;
    \item (hard to invert) for any adversary $\cA$, natural number $n$, and
      uniform random choice of input $x$ such that $|x| = n$, \[
        \Pr[f(\cA(1^n, f(x))) = f(x)] = \negl.
      \]
  \end{itemize}
  Note that $|x|$ here is \emph{not} the absolute value, but is instead the
  length of $x$ as a binary string: if $x$ is a number, then by encoding in
  binary have that $|x| = \Theta(\log_2 x)$.
\end{dfn}

The idea is that, given $y = f(x)$, $\cA$ attempts to find some $x'$ such that
$f(x') = y$. If some adversary can do this with non-negligible probability, then the
function is not one-way. While the probability must be negligible in $|x|$, the
adversary is given $f(x)$ and $1^n$ as an input, and hence must run polynomially
only in $|f(x)| + n$. This is a common technique called \emph{padding}, wherein
algorithms are given an extra input of $1^n$ to ensure they have enough time to
run.

We do not know that one-way functions exist. In fact, while the existence of
one-way functions implies that P $\neq$ NP, the converse is not
known\footnote{\cite{impagliazzo-1995} gives a classic discussion of the
  implications of various resolutions to P vs. NP on cryptography, including the
case where P $\neq$ NP but one-way functions nevertheless do not exist.}.
However, as in the following examples, we have excellent candidates under fairly
modest assumptions.

\begin{ex}[Factoring {\cite[subsection 2.3]{pass-shelat-2020}}]
  Suppose that for any adversary $\cA$ and for uniform random choice of primes
  $p,q<2^n$, \[
    \Pr[\cA(pq) = \{p, q\}] = \negl[n].
  \] This is the \emph{factoring hardness assumption}, for which there is
  substantial evidence. Then $(x,y)\mapsto xy$ is one-way\footnote{This
    statement is slightly imprecise: technically, $(x,y)\mapsto xy$ is \emph{weakly
    one-way}; to get the stronger notion of~\Cref{def:one-way function}
    requires a process called \emph{hardness
    amplification}. See~\cite[Section 2.4]{pass-shelat-2020}
for details.}.
\end{ex}

\begin{ex}[Discrete Logarithm {\cite[subsection 8.3.2]{katz-lindell-2014}}]
  Let $\{G_n\}$ be a fixed sequence of finite groups. The \emph{discrete logarithm hardness assumption}
  for $\{G_n\}$ is that, for any adversary $\cA$ and for uniform random choice of
  $g\in G_n$ and $h\in\<g\>$ such that $h = g^k$, \[
    \Pr[\cA(g,h) = k] = \negl[n].
  \]
  Under the discrete logarithm hardness assumption, $(g,k)\mapsto g^k$ is one-way.

  The discrete logarithm hardness assumption is known to be false for certain groups, such
  as the additive groups $\ZZ_p$ for prime $p$, in which case $g^k = gk$ and the
  Euclidean algorithm solves the problem. However, it is believed to hold for
  groups such as $\ZZ^*_p$. For a survey of various versions of this assumption,
  see~\cite{sadeghi-steinerr-2002}.
\end{ex}

\subsection{Proofs by Reduction}

Many cryptographic definitions, including \Cref{def:one-way function},
take the form \begin{quote}\emph{for any adversary $\cA$, natural number $n$, and uniform
random choice of input $x$ such that $|x| = n$, some predicate on the output
of $\cA$ has negligible probability.}\end{quote} The basic technique for proving results
using these definitions is called \emph{proof by reduction}. The idea is to
reduce one problem into another by starting with an arbitrary adversary
attacking the second and constructing an adversary attacking the first,
such that the probability of their successes is related. If we assume the first
problem is hard, then by studying the structure of the reduction we can learn
about the hardness of the second problem. As such, we often say that reductions
prove \emph{relative hardness results}, so that for instance \Cref{ex:reduction}
below proves the hardness of $g$ relative to $f$.


More specifically, to prove hardness of a problem $\Pi$ relative to $\Pi'$, a
proof by reduction generally goes as follows:
\begin{enumerate}
  \item Fix an arbitrary adversary $\cA$ attacking a problem $\Pi$.
  \item Construct an adversary $\cA'$ attacking a problem $\Pi'$ which:
    \begin{enumerate}
      \item Receives an input $x'$ to $\Pi'$.
      \item Translates $x'$ into an input $x$ to $\Pi$.
      \item Simulates $\cA(x)$, getting back an output $y$ which solves
        $\Pi(x)$.
      \item Translates $y$ into an output $y'$ which solve $\Pi(x')$.
    \end{enumerate}
  \item Analyze the structure of the translations to conclude that $\cA'$ solves
    $\Pi'$ with probability related to that with which $\cA$ solves $\Pi$.
  \item Given the hardness assumptions on $\Pi'$, conclude relative hardness of
    $\Pi$.
\end{enumerate}

The point is that $\cA'$'s job is to ``simulate'' the problem $\Pi$ to $\cA$,
using the data it gets from $\Pi'$ to construct an input to $\Pi$. We illustrate
this concept now.

\begin{ex}[a straightforward proof by reduction {\cite[subsection
  2.4.1]{pass-shelat-2020}}]\label{ex:reduction} Let $f$ be a one-way function. Then we claim $g:
  (x,y)\mapsto (f(x), f(y))$ is a one-way function. We can compute $g$ in
  polynomial time by computing $f$ twice, so it remains to show that $g$ is hard
  to invert.

  Let $\cA$ be any adversary. We will construct an adversary $\cA'$ such
  that, if $\cA$ can non-negligibly invert $g$, then $\cA'$ can non-negligibly
  invert $f$.

  The adversary $\cA'$ takes input $1^n$ and $y$. It then uniformly randomly
  chooses $u$ of length $n$ and computes $v = f(u)$, which is possible because $f$ is easy to
  compute. Now $\cA'$ computes $(u', x') := \cA(1^{2n}, (v,y))$ and outputs
  $x'$.

  When $\cA'$ simulates $\cA$, it passes $v$, which is $f(u)$ for a uniform
  random $u$, and $y$, which is (on well-formed inputs) $f(x)$ for a uniform
  random $x$. Thus, this looks like exactly the input that $\cA$ would
  ``expect'' to receive if it is attempting to break $g$. As such, whenever
  $\cA$ successfully inverts $g$, $\cA'$ successfully inverts $f$. Since
  everything is uniform we may pass to probabilities, and so: \begin{align*}
    \Pr &[g(\cA(1^{2n}, g(u, x))) = g(u, x)] \\
        &= \Pr[g(\cA(1^{2n}, (f(u), f(x)))) = (f(u), f(x))] &&\text{by definition of $g$}  \\
        &\leq \Pr[f(\cA'(1^n, f(x))) = f(x)] &&\text{by the above argument} \\
        &= \negl &&\text{by the hardness assumption for $f$}.
     \end{align*}

   Thus $g$ is one-way.
\end{ex}

Comparing this example to the above schema, we see that the problem $\Pi'$ is to
invert $f$, while the problem $\Pi$ is to invert $g$. The input $x'$ to $\Pi'$
is $y$, while the computed input $x$ to $\Pi$ is $(v, y)$. The output $y$ of
$\cA$ is $(x', u')$, while the computed output $y'$ is $x'$.

Diagramatically, we can represent the algorithm $\cA'$ as follows:
\[
  \begin{pic}
    \setlength\minimummorphismwidth{6mm}
    \node[morphism] (A) at (0,0) {$\cA$};
    \draw ([xshift=-2.5pt]A.south west) to ++(0,-1.5) node[left] {$y$};
    \draw ([xshift=-2.5pt]A.north west) to ++(0,1.05) node[left] {$x'$};
		\draw ([xshift=2.5pt]A.south east) to ++(0,-.5) node[state,scale=0.75] {\normalsize$\$$};
    \draw ([xshift=2.5pt]A.north east) to ++(0,.5) node[right] {};
    \draw[dotted] (-.8, -1.5) rectangle (.9, 1);
    \node at (1.2, -1.3) {$\cA'$\punctuation{.}};
  \end{pic}
\]

While this is not standard notation in cryptography, it will be useful for our future purposes.
We read these diagrams---called \emph{circuit} or \emph{string diagrams}---from
bottom to top. This diagram says that $\cA'$ is an algorithm which takes $y$,
uniformly randomly generates another input (this is what the $\$$ means), calls
$\cA$, and returns its first output.

\subsection{Computational Indistinguishability}

Computational indistinguishability formalizes the notion of two probability
distributions which ``look the same'' to adversarial processes. We begin with
probability distributions, but because we want to do asymptotic analysis, we
will eventually need to switch to working with sequences of probability
distributions.

\begin{dfn}[computational advantage]\label{def:computational advantage}
  Let $X$ and $Y$ be probability distributions over a set $A$. The \emph{computational
  advantage} of an adversary $\cD$, called the
  \emph{distinguisher}, over $X$ and $Y$ is \[
    \ca_\cD(X, Y) = \left|\Pr_{x\from X}[\cD(x) = 1] - \Pr_{y\from Y}[\cD(y) = 1]\right|.
  \]
\end{dfn}

The idea is that the distinguisher $\cD$ is trying to guess whether its input
was drawn from $X$ or $Y$; the computational advantage is how often it can do
so.

\begin{prop}\label{thm:advantage is metric}
  Let $\cD$ be a fixed distinguisher. Then $\ca_\cD$ is a
  pseudometric\footnote{
    A \emph{pseudometric} on a space $X$ is a function $d: X\times X\to\RR_{\geq 0}$ which
    is zero on identical points, symmetric, and satisfies the triangle
    inequality; in other words, it is a metric which does not necessarily
    differentiate distinct points.
  } on the
  space of probability distributions over an underlying set $A$.
\end{prop}

\begin{proof}
  Symmetry and non-negativity are immediate from the definition, while the
  triangle inequality follows from the triangle inequality for real numbers.
\end{proof}

We now turn to the asymptotic case.

\begin{dfn}[probability ensemble]\label{def:probability ensemble}
  A \emph{probability ensemble} is a sequence $\{X_n\}$ of probability
  distributions over sets $\{A_n\}$.
\end{dfn}

We say that two ensembles are computationally indistinguishable if there is no
efficient way to tell between them. Formally:

\begin{dfn}[computational indistinguishability]\label{def:computational indistinguishability}
  Two probability ensembles $\{X_n\}$ and $\{Y_n\}$ over a set $A$ are
  \emph{computationally indistinguishable} if for any (non-uniform PPT)
  distinguisher $\cD$ and any natural number $n$,
  \[
    \ca_\cD(X_n, Y_n) = \negl.
  \]
  In this case, we write $\{X_n\}\cind\{Y_n\}$.
\end{dfn}

\begin{rmk}
  A natural thought is to define a metric on probability distributions by
  \[
    \ca(X, Y) = \sup_{\cD}\ca_\cD(X, Y),
  \] and extend to ensembles by asking
  that $\ca(X_n, Y_n) = \negl$. Unfortunately, this does not quite yield the
  correct notion, as there exist ensembles which are computationally
  indistinguishable, but have sequences of distinguishers whose advantages for
  any fixed $n$ converge to $1$.
\end{rmk}

\begin{prop}
  Computational indistinguishability is an equivalence relation on the space of
  probability ensembles over a fixed set $A$.
\end{prop}

\begin{proof}
  Reflexivity and symmetry follow from the case of distributions. To show
  transitivity, let $\{X_n\} \cind \{Y_n\}$ and $\{Y_n\} \cind \{Z_n\}$. Let
  $\cD$ be any distinguisher. Then for any $n$, \begin{align*}
    \ca_\cD(X_n, Z_n) &\leq \ca_\cD(X_n, Y_n) + \ca_\cD(Y_n, Z_n) &&\text{by the triangle inequality} \\
                        &= \negl + \negl &&\text{by assumption} \\
                        &= \negl. &&\qedhere
  \end{align*}
\end{proof}

It is necessary to be precise about what is being claimed here. Transitivity
states that for any \emph{constant, finite sequence} of probability ensembles,
if each is computationally indistinguishable from its neighbors, then the two
ends of the sequence are computationally indistinguishable. In cryptography, we
sometimes want to consider the more general case of a countable sequence of
probability ensembles. We can do slightly better than the previous result:

\begin{prop}\label{thm:polynomial-indistinguishability}
  Let $\{X^k\}$ be a sequence of probability ensembles, so
  that each $X^k = \{X^k_n\}$ is itself a sequence of probability distributions,
  each over the underlying same sequence of sets $\{A_n\}$. Let
  $\{X^i\}\cind\{X^{i+1}\}$ for each $i$. Let $\{Y_n = X^{K(n)}_n\}$ for some
  polynomial $K$. Then $\{X^1_n\}\cind\{Y_n\}$.
\end{prop}

\begin{proof}
  Let $\cD$ be any distinguisher. Then for any $n$, \begin{align*}
    \ca_\cD(X^1_n, Y_n) &= \ca_\cD(X^1_n, X^{K(n)}_n) \\
                          &\leq \ca_\cD(X^1_n, X^2_n) + \cdots + \ca_\cD(X^{K(n) - 1}_n, X^{K(n)}_n) \\
                          &= K(n)\negl \\
                          &= \negl.
   \end{align*}
   In particular, the last equality follows because $K$ is polynomial.
\end{proof}

On the other hand, the result does not hold for arbitrary $K$. As we will see,
this is a fundamental limitation for cryptographic composition: we only expect
composition to work up to polynomial bounds.

One more closure result is valuable:

\begin{prop}\label{thm:ci-composition}
  Let $\{X_n\}\cind\{Y_n\}$, and let $\cM$ be a non-uniform PPT algorithm. Then
  $\{\cM(X_n)\} \cind \{\cM(Y_n)\}$.
\end{prop}

\begin{proof}
  The proof is by reduction. Let $\cD$ be a distinguisher. Then construct $\cD'$
  which, on input $x$, simulates $\cD(\cM(x))$. Then $\cD'$ outputs $1$ on $x$ if and
  only if $\cD$ outputs $1$ on $\cM(x)$, so \[
    \ca_\cD(\cM(X_n), \cM(Y_n)) = \ca_{\cD'}(X_n, Y_n) = \negl
  \] by the computational indistinguishability assumption.
\end{proof}

\begin{ex}[pseudorandom generators~{\cite[Sections 3.2-3.3]{pass-shelat-2020}}]\label{ex:pseudorandomness}
  We can use computational indistinguishability to formalize the notion of
  pseudorandomness.

  Let $\{\cX_n\}$ be a sequence of spaces, and let $\{X_n\}$ be a sequence of
  probability distributions over $\bigcup \cX_n$. We say that $\{X_n\}$ is
  \emph{pseudorandom for $\cX$} if there exists a polynomial $p$ such that \[
    \{X_n\} \cind \{\cX_{p(n)}\},
  \]where the latter is equipped with the uniform distribution. In other words,
  pseudorandom ensembles look uniformly random to distinguishers.

  For simplicity, we now work over $\cX_n = \ZZ_2^n$. Let $G:
  \ZZ_2^*\to\ZZ_2^*$ be a \emph{deterministic} function. We say $G$
  is a \emph{pseudorandom generator} if \begin{itemize}
    \item $G$ is polynomial-time computable;
    \item for any $x$, $|G(x)| > |x|$;
    \item $\{G(\ZZ_2^n)\}$ is pseudorandom.
  \end{itemize}
  The idea is that $G$ gets some input $x\in\ZZ_2^n$ and produces an output in
  $\ZZ_2^{p(n)}$ which looks uniformly random to distinguishers if $x$ is chosen
  uniformly at random. The polynomial $p(n)$ in the definition of
  pseudorandomness is now called the \emph{expansion factor}. It this sense, pseudorandom generators allow us to
  ``bootstrap'' randomness from random draws even on very small inputs.

  As usual, while we have excellent candidates, we have no proof that
  pseudorandom generators exist. However, there is a known procedure, due
  to \citeauthor{hastad-1999}~\cite{hastad-1999}, to turn any one-way function
  into a pseudorandom generator.
\end{ex}

\subsection{Interactive and Zero-Knowledge Computation}
\label{sec:interactive computation}
\label{sec:zero-knowledge}

Cryptographic protocols do not occur in a vacuum; instead, they rely on
computations involving multiple parties. We call such situations \emph{interactive
computations}. In general, a model of interaction depends on
the underlying model of computation; this is for instance the case with the
popular notion of interactive Turing machines~\cite[Definition
4.2.1]{goldreich-2001}. As our approach in this chapter has been
model-independent, we can only give an informal discussion of interaction.

An \emph{interactive computation} consists of a finite number of \emph{parties},
which we think of as algorithms $\cA_i$, who may potentially communicate by
sending messages to each other, and whose behavior may change in response to
messages they receive. An \emph{interactive protocol}
just consists of descriptions of some interactive algorithms $\<\cA_1, \dots,
\cA_N\>$.

We often think of interactive computations as being indexed by a \emph{security
parameter} $n\in\NN$. Instead of asking each algorithm to be polynomial-time in
its inputs, we ask it to be polynomial in $n$, with the stipulation that the
inputs themselves are no more than polynomial in $n$, so that each algorithm has
time to read its own inputs. Intuitively, the security parameter represents a
``tuning'' of the security of the system, so that a bigger $n$ incurs greater
computational cost but gives stronger security guarantees. Often, the security
parameter is formalized by ensuring that all parties get an extra input of $1^n$ at
the start of the computation, as we did in~\Cref{def:one-way function};
we assume this formalization in every protocol we give here.

At the start of an interactive computation, there is a \emph{global input} $x$
known to all parties, and each party $\cA_i$ may have a \emph{private} or
\emph{auxiliary input} $x_i$ known only to itself. We generally assume that
there are known sequences of \emph{input spaces} $\cX_n$ and $\cX^i_n$, such that
when the security parameter is $n$, $x\in \cX_n$ and $x_i\in \cX^i_n$. At the end of
the computation, each party may make some output, the sequence of which we
denote $\<\cA_1, \dots, \cA_N\>(x, x_1, \dots, x_N)$, so that party $i$'s output
is $\<\cA_1, \dots, \cA_N\>(x, x_1, \dots, x_N)_i$. When any of these algorithms
are potentially probabilistic, we think of this value as a distribution over
possible outputs, and we always assume that the internal randomness of the
parties is independent.

\begin{ex}\label{ex:swap}
  Here is a simple interactive protocol. We have two algorithms, $\cA$ and
  $\cB$. The input spaces are $\cX^\cA_n$ and $\cX^\cB_n$; there is no global
  input (which means the global input is just the security parameter $1^n$.) The
  algorithm $\cA$ takes its input $x\in\cX^\cA_n$, sends it to $\cB$, and outputs
  the first message it recieves from $\cB$. The algorithm $\cB$ takes its input
  $y\in\cX_n^\cB$, sends it to $\cA$, and outputs the first message it recieves
  from $\cA$. Then we have that $
    \<\cA,\cB\>(x,y) = (y,x).
  $
\end{ex}

The \emph{view} of a party is roughly all of the information it has available to
it over the course of the computation. This includes the global input, its
private input, any random bits it uses, and all the messages it receives. We
denote the view of party $i$ by $\view_i^{\<\cA_1, \dots, \cA_N\>}(x, x_1,
\dots, x_N)$. When the algorithms are clear from context, we may omit the
superscript. Importantly, while each private input $x_k$ is a parameter of each
view $\view_i$, the view does not necessarily include each of these inputs; they
are parameters merely because they may affect the messages received by party
$i$.

\begin{ex}
  In the protocol of \Cref{ex:swap}, \[
    \view_\cA^{\<\cA, \cB\>}(1^n, x, y) = \view_\cB^{\<\cA, \cB\>}(1^n, x, y) = \{1^n, x, y\}.
  \]
  Suppose that $\cB'$ always sends the string $0^n$ to $\cA$, instead of its input. Then \[
    \view_\cA^{\<\cA, \cB'\>}(1^n, x, y) = \{1^n, x, 0\},\quad\quad \view_{\cB'}^{\<\cA, \cB'\>}(1^n, x, y) = \{1^n, x, y\}.
  \]
  Notice that $\view_{\cB'}$ does not include the messages which it sends $\cA$.
\end{ex}

% There may be limitations on these messages: for instance,
% it may only be possible to send messages broadcasted to all the parties, or it
% may be possible to send ``private'' party-to-party messages. When necessary, we
% will always be clear about our assumptions here. 

The \emph{running time} of an interactive algorithm $\cA$ is now the function
$T_\cA: \NN\to\NN$ which, for any $n$, gives the maximum number of ``steps'' it
takes $\cA$ to halt over any choice of:
\begin{itemize}
  \item global input $x$ and private input $y$ of total length $n = |x| + |y|$;
  \item other algorithms involved in the computation;
  \item internal randomness of $\cA$ and of any other algorithms involved in the computation.
\end{itemize}
Essentially, when we say an algorithm is polynomial-time, we mean it is
\emph{always} polynomial-time, no matter what. We sometimes assume that each
algorithm has a ``clock'' that it uses to count the number of steps it has taken
and ensure it halts in some fixed polynomial number of steps.

We can now formalize the idea of a party ``learning something'' from an
interaction. We say that an interactive protocol $\<\cA_1, \dots, \cA_N\>$ is
\emph{zero-knowledge for party $i$} if there exists a non-uniform PPT algorithm
$\cS$ such that for any choice of inputs $(x, x_1, \dots, x_N)$, \[
  \cS(x, x_i) \cind \view_i^{\<\cA_1, \dots, \cA_N\>}(x, x_1, \dots, x_N).
\]
The idea is that the ``simulator'' $\cS$ gets only the inputs to $\cA_i$ and is
responsible for producing a distribution that is indistinguishable from the
actual view of $\cA_i$. If they can do this, then $\cA_i$ must not have learned
anything that they could not have computed directly from their inputs.

More often, we want to consider the situation where $\cA_i$ is supposed to learn
\emph{something} from the computation, but should not learn anything
\emph{extra}.

\begin{dfn}[zero-knowledge]\label{def:zero-knowledge}
  Let $f$ be a function. An interactive protocol $\<\cA_1, \dots, \cA_n\>$ is
  \emph{zero-knowledge for party $i$ relative to $f$} if there exists a
  (non-uniform PPT) simulator $\cS$ such that for any choice of
  inputs $(x, x_1, \dots, x_N)$, \[
  \cS(x, x_i, f(x, x_1, \dots, x_N)) \cind \view_i^{\<\cA_1, \dots, \cA_N\>}(x, x_1, \dots, x_N).
\]
\end{dfn}

In the above definition, we are asking that the simulator produces a
distribution which is negligibly close, in the sense of computational
indistinguishability, to the actual view. While this is all that is possible in
many situations in practice, we could ask for the stronger condition that the
produced distribution is \emph{identical} to the view. We call this notion
\emph{perfect} or \emph{information-theoretic} zero-knowledge, and refer to
\Cref{def:zero-knowledge} as
\emph{computational} zero-knowledge when we wish to emphasize the distinction.

\begin{ex}\label{ex:trivial-protocol}
  We show that the \emph{trivial protocol}, in which two algorithms $\cA$ and
  $\cB$ do nothing, is zero-knowledge for $\cB$. Our goal is to give a
  simulator $\cS$ such that for any choice of security parameter $n$,
  \[
    \cS(1^n) \cind \view_\cB^{\<\cA, \cB\>}(1^n).
  \] Since $\cB$ never gets sent any messages, its view is just the input
  $1^n$. We therefore let $\cS$ compute the identity, so that the two
  distributions are both constantly $\{1^n\}$. This shows that the trivial
  protocol is perfect zero-knowledge.
\end{ex}

\begin{ex}\label{ex:non-zero-knowledge}
  Consider the following protocol: $\cA$ gets input $x\in\ZZ_2^n$, which it
  then sends to $\cB$. To show this is not zero-knowledge for $\cB$, we must
  show that for any simulator $\cS$, there exists a choice of input $x$ and a
  distinguisher $\cD$ which distinguishes $\cS(1^n)$ from $\view_\cB(1^n, x) =
  \{1^n, x\}$ with non-negligible probability.

  Let $\cS$ be fixed. If $\cS$ does anything other than outputting $1^n$ and
  some $y\in\ZZ_2^n$, then we will be able to distinguish it syntactically.
  We may therefore safely assume that $\cS$ outputs $(1^n, y)$ for some
  (potentially random) choice of $y$. For each $n$, now choose $x$ such that
  $\Pr[y = x]\leq 2^{-n}$, which is possible by the pigeonhole principle. Let
  $\cD$ output $1$ on input $\{1^n, x\}$, and $0$ otherwise. Then \[
    \Pr[\cD(\cS(1^n)) = 1] = \Pr[y = x] \leq 2^{-n},
  \] while \[
    \Pr[\cD(\view_\cB(1^n, x)) = 1] = 1.
  \] Since $|1 - 2^{-n}|$ is not negligible, the protocol is not
  zero-knowledge for $\cB$.
\end{ex}

\subsection{Adversaries and the Real-Ideal Paradigm}
\label{sec:real-ideal}

Zero-knowledge is a surprisingly general tool for formalizing security
definitions, but in some cases it is not enough. For instance, we may want to
verify that protocols for electronic coin-flips are fair: in this case, the
issue is not that the parties may learn something extra, but that they may be
able to unduly influence the outcome of the computation. The general approach
taken in the literature is to define security on an ad-hoc basis for each such
task by enumerating the properties we want the protocol to have and formalizing
them as adversarial games. We will take a more systematic approach, sometimes
called the \emph{real-ideal paradigm}.

The idea is to define an \emph{ideal protocol}---also called an \emph{ideal
functionality}---which represents the desired behavior of the cryptosystem. The
protocol under study---the \emph{real protcol}---is then supposed to emulate the
ideal protocol, in the sense that its outcomes should be computationally
indistinguishable from the outcomes of the ideal protocol. The subtelty here is
our use of the term \emph{outcome}; which necessarily looks different for
different protocols: it may just be the information learned by a party, in which
case we recover zero-knowledge, it may be some function of the party's outputs,
or it may be something else altogether. We will see several different examples
of this in \Cref{sec:crypto-problems}, but the important point is that to define
security in this paradigm requires both a definition of the ideal protocol and
of the data to be compared.

Unlike in the case of zero-knowledge, where we only cared about what parties
could learn from the protocol assuming it was executed correctly, in this
setting we also want to discuss security against \emph{malicious behavior}, in
which one or more parties deliberately try to sabotage the outcome of the
protocol. We often refer to these parties as the \emph{adversaries}, and allow
them to be non-uniform even when ordinary parties in the protocol are uniform.
Since a protocol is not secure if even one possible attack is likely to succeed,
we say that a protocol is \emph{secure in the presence of malicious adversaries}
if for any choice of algorithms taking over some fixed number of parties, the
outcomes of the protocol are indistinguishable from the ideal. Again, we will
see examples of this notion in the next section.

In contrast, sometimes we do want to talk about a protocol being zero-knowledge
without dealing with arbitrary adversarial behavior. In this case, we use the
term \emph{semi-honest adversaries}, which informally refers to adversaries
which follow the prescriptions of the protocol, but attempt to learn as much as
they can within those bounds. There are several other notions of adversarial
strength considered in the literature---for instance, adaptive vs. static
adversaries~\cite{cramer-et-al-1999}, Byzantine
adversaries~\cite{lindell-et-al-2002}, and ``coercible''
parties~\cite{canetti-et-al-2015}. We do not explore all these models here.

\section{Cryptographic Problems}
\label{sec:crypto-problems}

\subsection{Encryption}

Much of the machinery defined in the previous section was originally developed
in the 1970s and 80s for the purpose of analyzing \emph{encryption problems},
culminating in the work of
\citeauthor{goldwasser-micali-1982}~\cite{goldwasser-micali-1982}. The idea of
an encryption problem is that a party Alice has a message $m$ in the
\emph{message space} $\cM_n$ which they want to send to Bob, but any message
they send to Bob must also be sent to the eavesdropping Eve. In the simpler
\emph{shared-key encryption problem}, which we consider here, Alice and Bob
share some secret key $k$ from the \emph{key space $\cK_n$}, which is unknown to
Eve.

% In this case, the problem reduces to a choice of three probabilistic
% polynomial-time algorithms\footnote{Here, as is general practice, we omit the
% dependence of the message and key spaces on the security parameter $n$.}:\begin{itemize}
%   \item $\alg{Gen}$, which takes as input a security parameter $1^n$ and
%     outputs a key $k\in\cK$;
%   \item $\alg{Enc}$, which takes as input a security parameter $1^n$, a key
%     $k\in\cK$, and a message $m\in\cM$, and outputs a ciphertext $c\in\cM$;
%   \item $\alg{Dec}$, which takes as input a security parameter $1^n$, a key
%     $k\in\cK$, and a ciphertext $c\in\cM$, and outputs a message $m\in\cM$.
% \end{itemize}

% An encryption scheme is \emph{correct} if for any choice of $m$ and $k$ output
% by $\alg{Gen}(1^n)$, \[
%   \alg{Dec}(1^n, k, \alg{Enc}(1^n, k, m)) = m.
% \]

% It is not hard to show that we may assume that $\alg{Gen}$ outputs a key chosen
% uniformly at random from $\cK$. As a result, and now using the language
% of~\Cref{sec:interactive computation}, we
% say that:

\begin{dfn}[shared-key encryption scheme]
Let $\{\cM_n\}$ and $\{\cK_n\}$ be sequences of sets. An $(\cM, \cK)$-shared-key
encryption scheme is an interactive protocol consisting of three interactive
algorithms $\cA$, $\cB$, and $\cE$, where:
\begin{itemize}
  \item the global input is $1^n$, the security parameter;
  \item $\cA$ gets a uniform random key $k\in\cK_n$ and a message $m\in\cM_n$ as private input;
  \item $\cB$ gets the same\footnote{
    Notice that this definition includes the stipulation that $\cA$ and $\cB$ share a
    uniform random key as input, which is not directly possible using the machinery
    of~\Cref{sec:interactive computation}.
    One way to formalize this notion is to add a fourth machine $\cG$ (the
    ``generator''), which can message $\cA$ and $\cB$ freely (but not recieve
    messages from them), whose job is to generate the key and send it to both
    parties. For our purposes, the important point is that while the input $m$
    is seen as a parameter of the system which can be controlled in
    indistinguishability proofs, the input $k$ is instead always randomly
    generated.
    } key $k$ as private input;
  \item $\cE$ gets no private input;
  \item $\cA$ and $\cB$ only send messages to each other if they also send the
    message to $\cE$.
\end{itemize}

A shared-key encryption scheme is \emph{correct} if $\cB$ outputs $m$ at the
end of the computation. A shared-key encryption scheme is \emph{secure} if it
is zero-knowledge for $\cE$; explicitly, if there exists a simulator $\cS$
such that for any choice of security parameter $n$ and message $m$, \[
  \cS(1^n) \cind \view_\cE^{\<\cA, \cB, \cE\>}(1^n, k, m),
\]where the randomness of the second distribution is over both the randomness of
the algorithms and uniform random choice of $k$.
\end{dfn}

The point is that the eavesdropper should learn nothing from the interaction,
while the intended recipient should learn the message.

\begin{ex}
  We can construct both a secure-but-not-correct and a correct-but-not-secure
  encryption scheme using work already done.
  \begin{itemize}
    \item For a secure-but-not-correct scheme, simply have each machine do
      nothing. The security proof is exactly the same as
      in~\Cref{ex:trivial-protocol}.
    \item For a correct-but-not-secure scheme, have $\cA$ send $m$ to $\cB$ (and
      therefore also to $\cE$) as a message, and have $\cB$ output that message. The
      insecurity proof is exactly the same as in~\Cref{ex:non-zero-knowledge}.
  \end{itemize}
\end{ex}

\begin{ex}[the one-time pad]\label{ex:otp}
We now give a secure and correct shared key encryption scheme, called the
\emph{one-time pad}. Let $\{G_n\}$ be a sequence of finite additive groups\footnote{
  We also want that $\{G_n\}$ is \emph{efficiently sampleable}, so that it is
  possible to generate an element from it uniformly at random in polynomial
  time.
} such that $|G_n| = \Omega(2^n)$, for instance $G_n = \ZZ_2^n$. We work over
$\cM_n = \cK_n = G_n$. Given a message $m$ and key $k$, $\cA$ computes $c = m +
k$, which it sends to $\cB$ (and $\cE$). $\cB$ then computes $c - k$, which it
outputs.

Correctness of this scheme is immediate, as $\cB$ outputs $c - k = m + k - k =
m$. To prove security, our goal is to construct a simulator $\cS$ such that
$\cS(1^n)$ is indistinguishable from $\view_\cE(1^n, k, m) = \{1^n, m + k\}$. Because
addition by $m$ is a bijection, and $k$ is chosen uniformly at random, the
distribution $\{m + k\}$ is just a uniform random sample from $G_n$. As such, we
simply let $\cS(1^n)$ draw $g$ uniformly at random from $G_n$ and output $\{1^n,
g\}$. This is again a perfectly-secure encryption scheme, since the two
distributions are identical.

Because the security is perfect, we don't need the asymptotics, so the one-time
pad is more commonly defined on a fixed group $G$, usually $\ZZ_2^m$ for some
fixed $m$.
\end{ex}

\begin{ex}[the bootstrap one-time-pad]\label{ex:botp}
  One disadvantage of the one-time pad is that the key must be drawn from the
  same space as the message. We now show how to rectify this, assuming a
  pseudorandom generator
  (\Cref{ex:pseudorandomness})
  $\cG$ with expansion factor $p$ is available.
  The idea is to use the pseudorandom generator to expand a short
    key into a longer one.

  The protocol is as follows. We let $\cK_n = \ZZ_2^n$ and $\cM_n =
  \ZZ_2^{p(n)}$. Given a message $m\in\ZZ_2^{p(n)}$ and a key $k\in\ZZ_2^n$,
  $\cA$ first computes $\cG(k)$ to get a key $k'\in\ZZ_2^{p(n)}$. It then sends
  $c = m + k$ to $\cB$. Similarly, $\cB$ computes $k'$ and then $c - k'$, which
  it outputs. Since $\cG$ is deterministic, both $\cA$ and $\cB$ get the same
  value for $k'$, and so the protocol is correct.

  Security can be shown by a reduction to the hardness assumption entailed by
  pseudorandomness of $\cG$, but an easier route is available. By definition of
  pseudorandomness, the distributions $\{\cG(\ZZ_2^n)\}$ and $\{\ZZ_2^{p(n)}\}$
  are computationally indistinguishable. To obtain $c$ in this protocol and in
  the one-time pad, we perform the same computation on these
  distributions---merely adding the fixed message $m$. As such,
  by~\Cref{thm:ci-composition} the view of $\cE$ in this
  protocol is indistinguishable from the view of $\cE$ in the one-time pad. Now
  we obtain the desired result by security of the one-time pad and transitivity
  of indistinguishability.
\end{ex}

\subsection{Interactive Function Computation}

% TODO: cite \cite{lindell-2017}.

Suppose we are given a (potentially stochastic) series of functions \[
  f: X_1\times\cdots\times X_N\to Y_1\times\cdots\times Y_N.
\] Each such function yields the following cryptographic problem\footnote{
  Here we need to assume that there is a canonical grading on each input set, for
  instance given by the length of a bitstring, so that each $X_i$ can also be
  thought of as a sequence of sets $\cX^i_n$ in the style of
  \Cref{sec:interactive computation}.
}:
\begin{quote}
  Can $N$ parties, each given a private input $x_i\in X_i$, work together so
  that the $i$th party outputs the value $f_i(x_1,\dots,x_n)$?
\end{quote}
Here, $f_i$ is the projection $\pr_i\circ f$. In particular:

\begin{dfn}
An interactive protocol $\<\cA_1,\dots,\cA_N\>$ \emph{computes the function $f$}
if for any choice of inputs $x_1,\dots,x_N$, \[ \<\cA_1, \dots,
  \cA_N\>(x_1,\dots,x_N) = f(x_1,\dots,x_N),
\] where if $f$ is stochastic the equality should be interpreted in the
distributional sense.
\end{dfn}

\begin{ex}
  The protocol of \Cref{ex:swap} computes the function $f(x,y) = (y,x)$.
\end{ex}

On the other hand, there are several possible notions of security in this
setting, roughly following the lines discussed
in~\Cref{sec:real-ideal}.
We first consider the more straightforward semi-honest case, in which we ask
that no party should learn anything from the computation other than the value
they are intended to output.

\begin{dfn}\label{def:semi-honest-secure}
  A protocol $\<\cA_1,\dots,\cA_N\>$ \emph{securely computes the function $f$ in
  the presence of semi-honest adversaries} if it computes $f$ and it is
  zero-knowledge for each $i$th party relative to $f_i$.
\end{dfn}

\begin{ex}
  Consider the following protocol for $f(x, y, *) = (*, *, xy)$: the first two
  algorithms each send their inputs two the third, which computes and outputs the
  product. This protocol is not secure in the presence of semi-honest
  adversaries, because the third party learns the two factors, not just the
  product; this insecurity can be proved very similarly to
  \Cref{ex:non-zero-knowledge}.

  On the other hand, consider the following protocol for $f(x, y) = (*, xy)$:
  the first algorithm sends $x$ to the second, which computes and outputs the
  product. This protocol is secure in the presence of semi-honest adversaries.
  The simulator for the second algorithm, which gets its input $y$ and output
  $xy$, is responsible for producing a distribution indistinguishable from $\{x,
  y\}$; it can do this by computing $xy / y$.
\end{ex}

\begin{ex}[oblivious bit-transfer]
  The \emph{oblivious bit-transfer problem} is as follows. Alice has two
  bits $b_1,b_2\in\ZZ_2$, and Bob has a query $\sigma\in\{1,2\}$.
  The goal is for Bob to learn the appropriate bit from Alice,
  without revealing which bit they asked for. We can formalize this
  using~\Cref{def:semi-honest-secure}: the problem is to securely
  compute \[
    f: \ZZ_2^2\times \{1,2\}\to \{*\}\times\ZZ_2,\quad\quad ((b_1,b_2), \sigma) \mapsto (*, b_\sigma)
  \]in the presence of semi-honest adversaries. While the solution is outside
  our scope, this is possible (under standard complexity-theoretic assumptions)
  via a protocol originally due to~\cite{even-et-al-1985}.
\end{ex}

The situation with malicious adversaries is much more complicated, but we will
not need all the details here; they can be found in, for instance,~\cite[Section
4]{lindell-2017}.

% \begin{dfn}
%   Let $f$ be an $N$-party function. The \emph{ideal protocol for computing $f$}
%   has $N$ parties $\cA_1,\dots,\cA_N$ and a \emph{trusted party} $\cT$,
%   proceeding as follows:
%   \begin{itemize}
%     \item $\cA_i$ gets $x_i$ as input and sends it to $\cT$;
%     \item $\cT$ computes $y_1,\dots,y_n = f(x_1,\dots,x_N)$ and sends $y_i$ to
%       $\cA_i$;
%     \item $\cA_i$ outputs the received value $y_i$.
%   \end{itemize}

%   The \emph{outcome of the ideal protocol} is the distribution \[
%     \ideal^\<\cA_1,\dots,\cA_N\>_f(x_1,\dots,x_N) =
%     \{\view^{\<\cA_1,\dots,\cA_N,\cT\>}_{\cA_i}(x_1,\dots,x_N): i\in\{1,\dots,N\}\}.
%   \]
% \end{dfn}

% \subsection{Commitment}

\subsection{Zero-Knowledge Proof}

In an \emph{interactive proof}, one party---the \emph{prover}---tries to
convince the other---the \emph{verifier}---that some statement is true. We
generally consider proofs of membership predicates over fixed sets (called
\emph{languages} in this context), so that the prover is trying to convince the
verifier that some global input $x$ is in a fixed set $\cL$. The key point is
that the prover generally has some computational advantage over the verifier, so
that the verifier cannot simply reproduce all the steps taken by the prover.

Informally, there are two correctness properties we want an interactive proof
system to have. It should be \emph{sound}: when given an input $x\in\cL$, the
verifier should except with high probability. It should also be \emph{complete},
which informally means that no adversarial machine should be able to convince
the verifier to accept an input not in $\cL$ except with low probability.
Formally:
\begin{dfn}[interactive proof system]\label{def:interactive-proof-system}
  Let $\cL$ be a fixed language. An \emph{interactive proof system for $\cL$}
  consists of an algorithm $\cP$ and a PPT algorithm $\cV$, such that:
  \begin{itemize}
    \item (soundness) for each $x\in\cL$, \[
        \Pr[\<\cP,\cV\>(x)_{\cV} = 1] > \frac{2}{3};
      \]
    \item (completeness) for each $x\notin\cL$ and each algorithm $\cP'$, \[
        \Pr[\<\cP',\cV\>(x)_{\cV} = 1] < \frac{1}{3}.
      \]
  \end{itemize}
\end{dfn}
Once we achieve any probability of success greater than $\frac{1}{2}$, we can
repeat the protocol to achieve any constant probability of success. It is
standard to choose $\frac{2}{3}$ as the desired bound for such cases.

\begin{ex}[graph non-isomorphism~{\cite[Section
  4.2.2]{goldreich-2001}}]\label{ex:graph-non-iso}
  We give an interactive proof system for the problem of \emph{graph
  non-isomorphism}\footnote{
    Since we do not know whether graph non-isomorphism is in NP, the reader may
    now wonder what the computational strength of interactive proof is. It turns
    out that IP, the class of languages with interactive proof, equals PSPACE,
    the class of languages whose membership predicates can be computed in
    polynomial space. This result was first proven by~\cite{shamir-1992}.
  }, which is formalized as the language \[
    \text{GNI} = \{(G_1,G_2): G_1\text{ and }G_2\text{ are non-isomorphic finite graphs}\}.
  \]
  The idea is that the verifier will construct a random graph isomorphic to one
  of the chosen graphs, and the prover will have to guess which one. If the
  graphs are non-isomorphic, then it should always be able to do so; if they
  are, it can only do so with probability $\frac{1}{2}$.

  The algorithms get a pair of graphs $(G_1,G_2)$ as shared input; we let $G_1 =
  (V_1,E_1)$ and similarly for $G_2$. The verifier uniform-randomly chooses $\sigma\in\{1,2\}$
  and a relabeling of $G_\sigma$. In particular, it uniform-randomly chooses a
  bijection $\pi: V_\sigma\to \{1,\dots,|V_\sigma|\}$ and generates the graph \[
    G_\sigma' = (\{1,\dots,|V_\sigma|\}, \{(\pi(v),\pi(w)): (v,w)\in E_\sigma\}),
  \]
  which it sends to the prover. The prover, which is unbounded, can then check
  whether $G_\sigma'\cong G_1$ or $G_\sigma'\cong G_2$ and send the result to
  the verifier; if both hold, then it sends a random bit. The verifier repeats
  the experiment again, outputting $1$ if the prover guessed right both times,
  and $0$ otherwise.

  To show soundness, note that if $G_1\not\cong G_2$, then the prover will always
  guess correctly, in which case the verifier outs $1$ with probability $1$.

  To show completeness, note that $G_\sigma'$ is a uniform random draw from \[
    \{G = (V,E): G\cong G_\sigma, V = \{1,\dots,|V_\sigma|\},
  \]
  i.e. from the isomorphism class of $G_\sigma$ with vertex set
  $\{1,\dots,|V_\sigma|\}$. Since $G_1$ and $G_2$ have identical isomorphism
  classes and the same number of vertices, this implies that the distribution of
  $G_\sigma'$ is independent of $\sigma$. Now let $\cP'$ be any algorithm which
  is given $G_1$, $G_2$, and $G'_\sigma$ and must guess $\sigma$. By
  independence, we have that
  \begin{align*}
    \Pr[\cP'(G'_\sigma) = \sigma] &= \sum_{\tau\in\{1,2\}}\Pr[\cP'(G'_\sigma) = \tau \text{ and } \sigma = \tau] \\
                                  &= \sum_{\tau\in\{1,2\}}\Pr[\cP'(G'_\sigma) = \tau]\Pr[\sigma = \tau] \\
                                  &= \frac{\Pr[\cP'(G'_\sigma\in\{1,2\})]}{2} \\
                                  &\leq \frac{1}{2};
  \end{align*}
  since the experiment repeats twice, we get a probability of
  $\frac{1}{4}<\frac{1}{3}$, as desired.
\end{ex}

In cryptography, we are especially concerned with \emph{zero-knowledge proofs},
which are meant to reveal no information other than the truth of the statement
under proof. We will need something slightly stronger than for multi-party
computation: because this protocol involves the prover giving information in
response to queries from the verifier (such as the query graph $G'_\sigma$ in
\Cref{ex:graph-non-iso}),
we will need to ensure that no verifier can learn anything extra from the
prover, even if they give different queries than the protocol prescribes.

\begin{dfn}[zero-knowledge proof]\label{def:zkp}
  An interactive proof system $(\cP, \cV)$ is \emph{honest-verifier
  zero-knowledge} it is zero-knowledge for $\cV$. It is
  \emph{semi-honest-verifier zero-knowledge}, or just \emph{zero-knowledge}, if
  for each non-uniform PPT $\cV'$, the protocol $\<\cP, \cV'\>$ is
  zero-knowledge for $\cV'$. It is \emph{black-box zero-knowledge} if the
  simulator can query $\cV'$ but must be defined independently of
  it\footnote{There appear to be several distinct choices in the literature for
    naming conventions, for the computational strength of the prover, and for
  the level of honesty of the verifier. Our naming convention
follows~\cite{vadhan-2007}; a good survey of various definitions
is~\cite[Section 3]{goldreich-1994}.}.
\end{dfn}

If one-way functions exist, then every problem in NP has a
zero-knowledge proof~\cite{gmw-1991}. In fact, rather remarkably, under the
same assumption \emph{any} language that admits an interactive proof admits a
zero-knowledge proof~\cite{ben-or-et-al-1990}.

\begin{ex}
  The protocol of \Cref{ex:graph-non-iso}
  is an honest-verifier zero-knowledge proof.
\end{ex}

\section{Composition}

\subsection{The Issues at Hand}
\label{sec:composition-issues}

We now wish to consider whether our security definitions are closed under composition
of protocols. There are many different issues to consider in stating such a
composition theorem, including:

\begin{questions}
  \item What kinds of protocols are being composed? Our security definitions do not
    capture security of arbitrary interactive processes, so either we will
    need a substantially more general definition or we will need to limit
    our composition theorem to a specific class of
    protocols.\label{q:composed-protocols}
  \item What does it mean to compose these protocols? It is not immediately
    clear how to compose arbitary interactive
    algorithms.\label{q:composition-meaning}
  \item What kind of composition is allowed? In particular, we can
    consider \emph{sequential composition}, in which only one protocol is
    ``running'' simultaneously, or \emph{parallel composition}, in which many
    protocols may be running simultaneously. To formally state a parallel
    composition theorem, we need to either specify a \emph{scheduling model} and
    deal with low-level issues like atomicity, or find some way to abstract over
    these details.\label{q:allowed-composition}
  \item What kind of security is being preserved? Given a security definition
    for the component protocols, we need some say to derive the security
    definition for the composite protocol.\label{q:preserved-security}
  \item What kinds of adversaries does the theorem handle? Composition theorems
    may look very different for security against uniform and non-uniform
    adversaries, for instance---these subtle issues can lead to very different
    results.\label{q:adversarial-model}
  \item What protocols are we allowed to compose with? We could be allowed to
    compose with arbitrary protocols, which might not even be secure, or only
    with other protocols we already know are secure.\label{q:compose-with}
  \item How many times we can compose---for instance, must it be constant in the
    security parameter?\label{q:count-compositions}
\end{questions}

As these questions demonstrate, composition theorems are really quite difficult
to state. However, even without a formal theorem in mind, we can make a few
concrete observations.

In regards to~\Cref{q:compose-with}, we need to have some kind of
idea of ``independence of state'' between two protocols before we can compose
them. For instance, we certainly should not be allowed to compose the one-time
pad with a protocol that publicly broadcasts the key.

In regards to~\Cref{q:count-compositions}, we should at best expect
to be able to compose polynomially many protocols in the security parameter.
This is in part for complexity reasons---a polytime turing machine cannot
simulate super-polynomially many protocols---but there is also a security
explanation: as a consequence of
\Cref{thm:polynomial-indistinguishability}, we should only
expect to be able to compose polynomially many computationally indistinguishable
distributions before losing the indistinguishability.

We do not make attempt to give a comprehensive review of the composition
theorems or counterexamples in the literature. Instead, our goal is to chart a
motivating path towards the theory of \emph{universal composability}, and then give a
sufficiently detailed overview of that theory to equip the reader to compare it
to the cryptographic models we will study
in~\Cref{chap:categorical-cryptography}.

\subsection{Composing Interactive Function Computations}
\label{sec:function-composition}

Possibly the easiest non-trivial composition theorem to state is for sequential
composition of interactive function computations, but already in this case we
will run into several fundamental issues.

Suppose we have two $N$-party protocols $\<\cA_1,\dots,\cA_N\>$ and
$\<\cB_1,\dots,\cB_N\>$ which securely compute the $N$-party functions $f:
X_1\times\dots\times X_N\to Y_1\times\dots\times Y_N$ and $g:
Y_1\times\dots\times Y_N\to Z_1\times\dots\times Z_N$ in the presence of
semi-honest adversries. We wish to use this data to construct a composite
protocol for the composite function $g\circ f$. The natural choice is to have
the parties first run the protocol for $f$, then run the protocol for $g$. Thus,
we can state the following claim:

\begin{claim}
  Suppose $\{f^i: X^i_1\times\dots\times X^i_N\to X^{i+1}_1\times\dots\times
  X^{i+1}_N, 1\leq i \leq K\}$ is a finite sequence of functions which are securely
  computable in the presence of semi-honest adversaries. Then the composite $f =
  f^K\circ\dots\circ f^1$ is securely computable in the presence of semi-honest
  adversaries.
\end{claim}

\begin{proof}[Proof attempt]
  For each $i$, let $\Pi_i = \<\cA^i_1,\dots,\cA^i_N\>$ be a protocol which
  securely computes $f^i$ in the presence of semi-honest adversaries. Form a new
  protocol $\Pi$ which performs each of the $\Pi_i$ in sequence. Correctness is
  immediate, since the correctness of each $\Pi_i$ implies that $\Pi$ computes
  each step of the composite in turn.

  The proof of security is by reduction to the security of the $\Pi_i$. By
  induction, it suffices to consider the case $K = 2$. Suppose that the
  composite $\Pi$ does not securely compute $f^2\circ f^1$. Without loss of
  generality, suppose that the composite is not zero-knowledge for party $1$.
  Then for any $\cS$, there exists some non-uniform PPT $\cD$ which
  distinguishes, for any choice of inputs $x_i\in X^1_i$,
  \begin{equation}\label{eqn:failed-proof-1}
    \cS(x_1, f^2_1(f^1(x_1,\dots,x_N)))\quad\quad\text{ and }\quad\quad\view_1^{\Pi}(x_1,\dots,x_2).
  \end{equation}

  All this works, but we run into issues when we attempt to construct a
  simulator which contradicts this assumption. Let $\cS^1_i$ and $\cS^2_i$ be
  the simulators which witness security of $\Pi_1$ and $\Pi_2$ with respect to
  party $i$. We would like to construct a simulator $\cS$, which combines the
  simulated views of party $i$ in both parts of the computation. The issue is
  that, in order to fit the form of \eqref{eqn:failed-proof-1},
  $\cS$ should get only $(x_1, f^2_1(f^1(x_1, \dots, x_N)))$ as input. However,
  to be able to run $\cS^1_1$, it needs $f^1_1(x_1, \dots, x_N)$, which we have
  no way to obtain.
  \renewcommand{\qedsymbol}{}
\end{proof}

I am not aware of a direct way to repair this proof. Instead, to avoid the
issue, we need the structure of the composite protocol to mirror the structure
of the reduction: there should be some ``outermost'' protocol which handles
calls to the sub-protocols, just as we need to make an outermost simulator which
calls the sub-simulators. The right tool for the job is the \emph{oracle
algorithm}.

\begin{dfn}[oracle algorithms; oracle protocols]
  An \emph{oracle algorithm} is an algorithm $\cA$ equipped with a ``slot'' for
  an \emph{oracle} $\cO$, to which it can make \emph{queries} $x$ and receive
  \emph{responses} $\cO(x)$. We write $\cA^\cO$ to refer to the
  oracle-algorithm $\cA$ equipped with the specific oracle $\cO$.

  An \emph{oracle protocol} is a protocol $\<\cA_1,\dots,\cA_N\>$ with a slot
  for an oracle $\cO$, where each $\cA_i$ may write queries $x_i$, and if each
  machine does so, they each receive outputs $\cO(x_1,\dots, x_N)_i$. We write
  $\<\cA_1,\dots,\cA_N\>^\cO$.

  Let $f: X_1\times\dots\times X_N\to Y_1\times\dots\times Y_N$ be a function.
  An \emph{$f$-oracle} is an oracle which, when queried with $x_1,\dots,x_n$,
  responds with $f(x_1,\dots,x_N)$. We overloadingly write $f$ to refer to an
  $f$-oracle.
\end{dfn}

The idea is to make an oracle protocol $\Pi$ which, when instantiated with an
$f$-oracle, securely computes $g$. Then if we have a protocol for securely
computing $f$, we will show that we can substitute it for the oracle in $\Pi$.
In particular, while we will not be precise about what secure oracle computation
means, it is important that the view of an oracle algorithm includes its queries and responses.

\begin{dfn}[oracle reduction]
  Let $f$ and $g$ be functions. Then $g$ \emph{securely oracle-reduces to $f$ in the presence of semi-honest
  adversaries} if there exists an oracle protocol $\<\cA_1,\dots,\cA_N\>$ such
  that $\<\cA_1,\dots,\cA_N\>^f$ securely computes $g$ in the presence of
  semi-honest
  adversaries.
\end{dfn}

\begin{thm}[{\cite[Theorem 7.3.3]{goldreich-2001}}]\label{thm:oracle-sequential-composition}
  Suppose $g$ securely reduces to $f$ in the presence of semi-honest
  adversaries, and $f$ is securely computable in the presence of semi-honest
  adversaries. Then $g$ is securely computable in the presence of semi-honest
  adversaries.
\end{thm}

\begin{proof}[Proof sketch]
  Let $\Pi_g = \<\cA_1,\dots,\cA_N\>$ be an oracle protocol such that
  $\<\cA_1,\dots,\cA_N\>^f$ securely computes $g$ in the presence of semi-honest
  adversaries, and let $\Pi_f = \<\cB_1,\dots,\cB_N\>$ be a protocol which securely
  computes $f$ in the presence of semi-honest adversaries. We construct a new
  protocol $\Pi$ which runs $\Pi_g$, but whenever the oracle is queried, it
  instead replaces the oracle with a complete run of $\Pi_f$. Correctness of
  $\Pi$ is immediate from its construction; we show security by a reduction.

  Let $\cS^g_i$ simulate the view of party $i$ in $\Pi_g^f$, and let $\cS^f_i$
  simulate the view of party $i$ in $\Pi_f$. We construct a simulator $\cS_i$
  for the view of party $i$ in $\Pi$. Since the inputs to $\Pi_g^f$ are the same
  as to $\Pi$, we can immediately run $\cS^g_i$ to obtain the view of party $i$
  in the computation $\Pi_g^f$, which in particular includes the queries and
  responses of the oracle. We can then pass these queries and responses to
  $\Pi_f$, obtaining a view of each of the invocations of the sub-protocol.

  To show the this simulator achieves the necessary indistinguishability result,
  we work in two steps. First, the output of $\cS_i$ is indistinguishable from
  the view of party $i$ in $\Pi$ with the sub-protocol invocations are
  replaced with the simulated views; we prove this by the security of $\Pi_f$,
  since otherwise appending the view from $\Pi_g^f$ would distinguish the view
  from $\Pi_f$ with its simulated counterpart. Similarly, this latter distribution
  is indistinguishable from the view of party $i$ in $\Pi$ without any such
  replacement, this time by security of $\Pi_g^f$. Filling in these details
  completes the proof.
\end{proof}

A few remarks on this proof are warranted. First, it fundamentally relies on the
semi-honesty of the adversaries, since it means that the execution of the
sub-protocols is completely independent from anything other than their inputs.
If the adversaries could behave maliciously, and thus use information learned in
the larger protocol to affect run the sub-protocols, then the main simulator would be
unable to properly invoke the simulator for the sub-protocols.

Second, these techniques seem completely unable to handle parallel composition.
The issue is that oracle queries are in some sense immediate---the assumption is
that everything else pauses while the oracle does its work. It is much more
difficult to handle parallelism, or even more strongly, to handle concurrency,
which may in particular be asynchronous.

Third, if we want to handle protocols other than function computations, we will
need a more robust notion of composition of ideal functionalities---this proof
is tied to the easy-to-understand structure of function computation.

The goal of universal composability is to resolve these issues, but first we
will explicitly give an example wherein parallel composition fails.

\subsection{A Counterexample to Parallel Composition}
\label{sec:no-parallel-zk}

Before giving the counterexample, we briefly discuss sequential composition of
zero-knowledge proofs, which is quite subtle. When the definitions are as given
in~\Cref{def:zkp}, black-box zero
knowledge proofs do compose in sequence~\cite{goldreich-1994}. On the other
hand, if we change the definitions so that both the simulator of
~\Cref{def:zero-knowledge} and the
adversarial verifier of~\Cref{def:zkp}
are required to be \emph{uniform} PPT,
then computational zero-knowledge does not
compose~\cite{goldreich-krawczyk-1996}. However, if under the same definition we
require that the prover is in NP, then uniform computational zero-knowledge
proofs do compose in sequence up to a constant number of times; but if the
distinguishers in the definition of computational indistinguishability are
required to be uniform, then sequential composition again fails~\cite{bv-2010}.
There are a huge number of variations to consider and I believe there are many
definitions for which the question of sequential composition is still open.

For parallel composition, the situation is much simpler: we give an argument due
to~\cite{goldreich-krawczyk-1996} that zero-knowledge proofs do not compose in
parallel. The idea of the counterexample is as follows. In proof A, the prover
poses a randomly chosen computationally intractable challenge to the verifier,
and then gives the verifier knowledge if and only if the verifier can solve the
challenge. This proof is zero-knowledge because PPT verifiers can solve the
challenge only negligibly often. In proof B, the verifier poses a challenge to
the prover, which the prover answers. The trick is to choose a class of
challenges whose answers are pseudorandom, so that the answer to the challenge
on its own does not carry knowledge. However, when proof A and proof B are run
together in parallel, the verifier can take the challenge it gets in proof A,
get an answer in proof B, and that use that to get knowledge from the prover in
proof A.

The difficulty in formalizing this is to find a class of challenges which are
computationally intractable, in that PPT algorithms solve them with negligible
probability; zero-knowledge, in that answers look pseudorandom; and decidable, in
that unbounded algorithms can answer challenges and check answers. Our
challenges will be phrased as sets: for each security parameter $n$, we will
agree to some set of sets $S^n_1, \dots, S^n_{2^n}\subseteq \bin^{Q(n)}$, where
$Q$ is a polynomial. A challenge looks like a value $i\in 1, \dots, Q(n)$, and a
solution is a value $s\in S^n_i$. We now formalize our desiderata as follows.

\begin{dfn}
  A \emph{non-uniform ensemble} is a sequence $\{S^n\}$, where for each $n$,
  $S^n = \{S^n_1, \dots, S^n_{2^n}\}$ is a set of $2^n$ sets. A non-uniform
  ensemble is:
  \begin{itemize}
    \item \emph{polynomially-sized} if there exists a polynomial $Q$ such that
      for each $n$ and $i$, $S^n_i\subseteq \bin^{Q(n)}$;
    \item \emph{pseudorandom} if for each $n$ and $i$, $S^n_i$ is pseudorandom
      (in the sense of \Cref{ex:pseudorandomness});
    \item \emph{decidable} if there exists an algorithm which, on input $(n,
      i)$, outputs the elements of $S^n_i$ and then halts;
    \item \emph{evasive} if for any non-uniform PPT algorithm $\cA$, \[
        \Pr_{i\in\{1,\dots,2^n\}}[\cA(i)\in S^n_i] = \negl.
      \]
  \end{itemize}
\end{dfn}

\begin{thm}[{\cite[Theorem
  3.2]{goldreich-krawczyk-1996}}]\label{thm:evasive-sets}
  There exists a polynomially-sized, pseudorandom, decidable, evasive
  non-uniform ensemble.
\end{thm}

Now the actual construction is relatively straightforward. Let $S$ be the
ensemble from \Cref{thm:evasive-sets}
and $Q$ the
size polynomial. Let $K$ be a computable predicate which is known not to be in
BPP; such things exist by the time hierarchy theorem from complexity theory. We
will give two zero-knowledge proofs for the language $\bin^*$.

Proof A goes as follows: on input $x\in\bin^n$, $\cP_A$ chooses $i\in\{1,\dots,2^n\}$ uniformly
at random, which it sends to $\cV_A$. Next, $\cV_A$ chooses $s\in\bin^{Q(n)}$
uniformly at random, which it sends to $\cP_A$. If $s\in S^n_i$, $\cP_A$ sends
$K(x)$ to $\cV_A$. Then $\cV_A$ outputs $1$. This proof is zero-knowledge
because the probability that any cheating $\cV'_A$ sends an $s\in S^n_i$ is
negligible, so the probability it learns $K(x)$ is negligible.

Proof B goes as follows: on input $x\in\bin^n$, $\cV_B$ chooses
$i\in\{1,\dots,2^n\}$ uniformly at random, which it sends to $\cP_B$. Next,
$\cP_B$ sends $s\in S^n_i$ to $\cV_B$. Then $\cV_B$ outputs $1$. 
This proof is zero-knowledge because $S^n_i$ is pseudorandom, so the simulator can
just output a uniform random value and by definition of pseudorandomness no
distinguisher can tell whether it is seeing that random value or the value from
$S^n_i$.

However, in the parallel composition, the verifier can wait to send $i$ until it
receives the $i$ from $\cP_A$. Then, when it gets back $s\in S^n_i$ from
$\cP_B$, it can send $s$ to $\cP_A$ and get back $K(x)$, and hence it learns
$K(x)$ with probability $1$.

Under computational hardness assumptions, \Cref{thm:evasive-sets}
can be extended to,
for instance, evasive ensembles which are decidable in NP, and hence under such
assumptions zero-knowledge proofs with NP provers also do not compose in
parallel.

As a consequence of this construction, it is clear that we need more technology
to handle parallel composition in any reasonable way. By far the most popular
attempt to do so is universal composability.

\subsection{Universal Composability}

In
\Cref{sec:function-composition},
we identified three critical issues with generalizing the proof of
\Cref{thm:oracle-sequential-composition}:
we need to handle malicious adversaries which can forward their views to each
other; we need to handle parallel composition during which other protocols can
run; and we need a very general way to describe composition and security of a
large class of computational protocols. Universal composability (UC), due to Ran
Canetti~\cite{canetti-2000,canetti-2020}, resolves all of these.

The single big idea of UC is to strengthen the notion of emulation required in
the real-ideal paradigm. Whereas our formulations of simulation security require
only that \emph{by the end of the computation} the real protocol produces an output
indistinguishable from the ideal protocol, in UC, this indistinguishability must
hold \emph{throughout} the computation. Intuitively, we should expect this to be
strong enough to allow us to prove a parallel composition theorem, because
in particular the other computation we are composing with will be unable to
distinguish our protocol from the ideal. In this section, we will sketch some of
the key tools used to make this formal, without attempting to be fully formal
ourselves.

In a UC proof, an algorithm called the \emph{environment} represents all the
other computational processes happening along with the protocol under study. The
environment is also responsible for giving inputs to parties in a protocol.
Ultimately, the environment is also the distinguisher, responsible for
attempting to determine between the real or ideal protocol. If it cannot do so,
then no other protocol is able to do so, so in particular the security
guarantees will hold no matter what else is going on. In this way, environments
abstract over a lot of the complexity in ordinary cryptographic definitions.

Adversarial behavior is somewhat complicated in UC. In addition to the
environment, there is an algorithm called the \emph{adversary} which may write
to the \emph{backdoor tapes} of all the parties involved in the protocol. The
protocol is responsible for specifying how the parties should behave in respnose
to messages writen on their backdoor tapes.

When $\pi$ is a protocol and $\cA$ and $\cE$ are algorithms, we write
$\exec_{\pi,\cA,\cE}$ for the output of $\cE$ after an interactive computation
with all the algorithms in $\pi$ and with the adversary $\cA$. A protocol $\pi$
\emph{UC-emulates} another protocol $\phi$ if for any adversary $\cA$ there
exists a simulator $\cS$ such that, for any environment $\cE$, \[
  \exec_{\pi,\cA,\cE}\cind \exec_{\phi,\cS,\cE}.
\]
The idea is that $\cE$ is trying to output a guess of whether it is interacting
with $\pi$ or $\phi$; it should be able to guess right only negligibly better
than half of the time.

We need a way to express ideal protocols for general cryptographic resources; UC
calls these \emph{ideal functionalities}. An ideal protocol for a functionality
essentially consists of an algorithm $\cF$, which receives its inputs from some
``dummy parties'' and then sends them the correct outputs back. In order to work
with more than just function computation, $\cF$ is also constantly talking to
the adversary, and so can simulate functionalities like public commitments that
do not show up in a function signature. A protocol \emph{UC-realizes} the
functionality $\cF$ if it UC-simulates the ideal protocol for $\cF$.

The next issue is a formal notion of composition. This should look like the
``oracle substitution'' construction from the proof
of~\Cref{thm:oracle-sequential-composition},
but handle more general forms of composition than the sequential composition
implied by the oracle reductions. Pick a protocol $\pi$ and a subset $\rho$ of
the parties involved in $\pi$. Pick another protocol $\phi$ such that there is
an injective map from the parties in $\rho$ to those in $\phi$. Then the
protocol $\pi^{\rho\to\phi}$ replaces $\rho$ with $\phi$, wiring up the
communication with the rest of $\pi$ according to the injection (note that
$\phi$ may have more machines than $\rho$; these do not talk to the other
parties in $\pi$). This is called the \emph{universal composition operation.} It
takes rather a lot of work to make this substitution operation precise---for
instance, there is a very technical compatibility condition which essentially
asks that the interfaces of $\phi$ and $\rho$ are ``functional'', in the
intuitive sense that they only interact with the rest of the protocol via their
inputs and outputs (and maybe as a result of corruption).
In~\cite{canetti-2020}, such protocols are called \emph{subroutine respecting}.

\begin{thm}[the univeral composition theorem{~\cite[Theorem 22]{canetti-2020}}]
  Under technical assumptions, if $\rho$ is a subroutine of $\pi$ and $\phi$
  UC-emulates $\rho$, then $\pi^{\rho\to\phi}$ UC-emulates $\pi$. In particular,
  if $\cF$ and $\cG$ are ideal functionalities such that $\pi$ UC-realizes
  $\cG$ and $\phi$ UC-realizes $\cF$, then $\pi^{\cF\to\phi}$ UC-realizes $\cG$.
\end{thm}

All this is just a sketch; there are of course many technical details to work
out. For instance, UC comes with its own entire low-level model of distributed
computation. The underlying machine model is a specific kind of
\emph{interactive Turing machine}, which has seven different tapes with
differing semantics and read/write permissions, and the network is mediated by a
\emph{control function}, which can modify or block messages between machines
involved in a computation. There is no formal notion of a ``secure channel'' in
UC, nor of other basic cryptographic resources; instead every resource is meant
to be represented as an auxiliary machine which implements some ideal
functionality.

To summarize and begin to evaluate the framework, we now return to the questions
from \Cref{sec:composition-issues}.

In \Cref{q:composed-protocols}, we asked what kinds of protocols are
being composed. Protocols in UC are sets of interactive Turing machines behaving
according a highly specific execution model. On one hand, Turing machines are a
standard and well-understood model of classical computation, and the flexibility
of the notion of environment allows for encoding a wide variety of security
properties. On the other hand, the technical specificity of the interaction
model makes it difficult to formally apply the model; in practice papers tend
not to work with the precise model, and it is not clear to me that the model is
sufficiently well-understood to justify the common level of hand-waving.
Furthermore, this machine model makes it difficult to apply UC to other models
of computation. For instance, to deal with quantum cryptography, we need to use
a mathematically separate framework based on quantum which redoes much of the
work of the UC paper---in fact, there are at least three competing frameworks
for doing so~\cite{unruh-2004,benor-2004,unruh-2010}, each with their own ad-hoc
notion of ``quantum machine.''

In \Cref{q:composition-meaning,q:allowed-composition}, we
asked about the definition and scope of composition. The subroutine substitution
operation from UC is extremely general. Canetti argues, and the last twenty
years have demonstrated, that together with control functions which model
adversarial network conditions, UC composition can handle sequential, parallel,
concurrent, and asynchronous composition; that it can handle compositions with
variable numbers of rounds and subroutine calls; with coordinated or
uncoordinated timings; with adaptively chosen inputs and adversaries;
composition with shared and independent state; and more. One good source of
examples of these claims is~\cite{canetti-2006}.

In \Cref{q:preserved-security}, we asked how the system derives
security definitions for composite protocols. UC works similarly to the oracle
model: there is one ``outermost'' protocol, and we substitute real protocols for
idealized subroutines within the protocol. The key point is the definition of
UC-emulation is strong enough to allow a protocol to be substituted for an ideal
functionality virtually anywhere and any time. However, this imposes a high
proof burden which makes many UC proofs intractable, as even aside from the
low-level details of the machine model, constructing a simulator that is
in constant conversation with the environment is quite burdensome. The high
proof burden also leads to impossibility results that can require somewhat ad-hoc
setup assumptions to
overcome~\cite{canetti-fischlin-2001,barak-et-al-2004,kidron-lindell-2007,jost-2020}.

In \Cref{q:adversarial-model}, we asked which kinds of adversaries the
system accommodates. UC is again quite flexible in this regard. Adversarial
behavior is built into the protocol via backdoor tapes and corruption messages,
and then a separate party called the adversary activates (potentially in a
controlled, stateful way, so that they cannot for instance just corrupt all the
parties) corruption messages during the protcol. In this way, UC conveniently
avoids dealing in the formalism with different models of adversaries, since they
may be specified on a protocol-by-protocol basis. For instance, a protocol meant
to be secure against semi-honest adversaries may just allow backdoor messages
which ask the corrupted party to forwards its state to the environment, while a
protocol meant to be secure against malicious adversaries may allow a party to
be completely piloted over via instructions sent to its backdoor tape.
In~\cite[Section 7.1.1]{canetti-2000}, Canetti gives examples of a wide variety
of adversarial models which can be incorporated in this way. Furthermore, this
approach allows UC proofs to compose protocols which are secure against
different forms of adversary.

In \Cref{q:compose-with}, we asked what protocols we can securely
compose secure protocols with. UC does not require that the protocols we compose
with are secure; in fact there are essentially no requirements on the ``outer''
protocol into which the substitution will occur. We also saw previously that any
framework for composition needs some way to assert that the protocols being
composed are ``independent enough'' that one does not give away the secret key
of the other; UC does this with the notion of ``subroutine respecting''
protocols, which are one of the only technical limits on composition.

In \Cref{q:count-compositions}, we asked how many times we can compose
protocols. We have not discussed nested UC-composition, but the universal
composition theorem holds up to polynomially many nested substitutions of
subroutines, which as discussed earlier is a tight bound.

% How does UC resolve the \cite{goldreich-krawczyk-1996} construction from
% \Cref{sec:no-parallel-zk}?
% The the adversary, which is talking to the verifier via
% the backdoor tape, sees the ``extra'' interactions between the verifier and the
% prover , which are not 
% to learn $K(x)$.
% Proof B is not 
%
Finally, we give two pieces of evidence of the naturality of UC.
Technically,~\cite{lindell-2003} proved that, if a protocol for multi-party
function computation is secure under parallel composition with even a constant
number of (not-necessarily-secure) other protocols, then it is UC-secure.
Socially, a precise connection has recently been established between UC and the
independently-formulated \emph{robust compilation} (RC) framework from
programming language security~\cite{patrignani-2022}; while RC is a
comparatively new tool, this connection suggests a cross-field applicability of
even the technical details of UC.

That said, while UC has clear merits, especially in terms of its incredible
flexibility, there are serious disadvantages to carrying around that degree of
complexity. In particular, UC cannot handle other models of computation, its
proofs are often messy and hard to verify, and it carries several
impossibility results that seem somewhat artificial. We will conclude the
chapter by surveying some alternatives.

\subsection{Alternative Approaches}

Several authors attempt to simplify UC by reducing the intended scope. This
approach is most notably taken by the ``simple UC'' approach of~\cite{ccl-2015}
in the special case of standard multiparty computation, as well by ``simplified
UC''\footnote{
  Confusingly, not only are simple UC and simplified UC unrelated, the abstract
  of~\cite{ccl-2015} actually refers to their framework as simplified UC,
  whereas in the paper and the rest of the literature it is called simple UC.
  The simple UC model of~\cite{ccl-2015} is a better-known
framework and references to this term in the literature generally refer to this
model.}~\cite{wikstr-2016}, which fixes the number of parties and does not handle
adaptive adversaries. The approach of~\cite{ccl-2015} has been quite successful;
simple UC has been widely used for composable proofs of secure multiparty computation
in the literature~\cite{mansy-2019,hazay-2020,lindell-2022,scheffler-2023}.

Another approach in the direction of UC is to use proof automation technologies
from programming language theory to give constructive UC proofs which can be
checked and even implemented by machine. For instance,
IPDL~\cite{morisett-et-al-2021}, a logic for reasoning about secure
probabilistic message-passing computations, has a weak equational theory
generated by a UC-inspired observational equivalence relation, while symbolic
UC~\cite{bohl-unruh-2013} and the interactive lambda
calculus~\cite{liao-et-al-2019} build programming calculi for UC variants over
the $\pi$-calculus. There has even been early progress, in the form of
EasyUC~\cite{canetti-et-al-2019}, in implementing domain-specific languages for
frameworks of this sort in proof assistants. While these approaches generally
trade complex machine models for complex typing disciplines without reducing the
complexity off the core framework, the hope is that these typing disciplines
will facilitate easier proof automation, removing a lot of the complexity from
human view.

Slightly further from UC, several authors give UC-like frameworks with different
low-level machine models. This approach is taken, for instance, by IITM-based
UC~\cite{camenisch-et-al-2019}, which uses ``interactive inexhaustable Turing
machines'', and GNUC~\cite{hofheinz-shoup-2011}, which uses statically-linked
composition rather than the dynamic linking of UC. None of these alternative
models have caught on to any significant degree; they generally seem to suffer
either from expressiveness issues or from the same overcomplexity as UC.

A more radical approach is to ignore the low-level details entirely, and
instead give an algebraic axiomatization of the properties which a machine model
ought to satisfy. This approach is taken by both constructive
cryptography~\cite{maurer-et-al-2012} and abstract
cryptography~\cite{maurer-renner-2011}.  In these closely-related models, there
is an abstract notion of a ``resource system,'' which is a partially ordered set
of resources and set of reductions between them satisfying some axioms. These
algebraic theories can then be instantiated explicitly with resources and
reducations obtaind from some specific class of cryptographic systems.

As this brief survey demonstrates, the problem of cryptographic composability is
an active and important area of research with many different ongoing approaches.
An excellent high-level summary of the state of the field (as of 2019) is the
report from the Dagstuhl seminar on the subject~\cite{camenisch-et-al-2019}.
